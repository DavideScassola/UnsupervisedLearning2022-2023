{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "using DataFrames\n",
    "using LinearAlgebra\n",
    "using Statistics\n",
    "using Plots\n",
    "using Random\n",
    "using GLM\n",
    "import CSV\n",
    "using FreqTables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloading and extract the dataset\n",
    "X_file = \"data_kPCA_2022-2023.txt\"\n",
    "y_file = \"labels_kPCA_2022-2023.txt\"\n",
    "url = \"https://raw.githubusercontent.com/alexdepremia/Unsupervised-Learning-Datasets/main/\"\n",
    "\n",
    "if !isfile(X_file)\n",
    "    run(`wget $(url*X_file)`)\n",
    "end\n",
    "if !isfile(y_file)\n",
    "    run(`wget $(url*y_file)`);\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the dataset (X, y)\n",
    "X = Float32.(Matrix(DataFrame(CSV.File(\"data_kPCA_2022-2023.txt\", delim=\" \", header=false))))\n",
    "y = parse.(Int8, readlines(open(\"labels_kPCA_2022-2023.txt\")))\n",
    "\n",
    "#=\n",
    "println(\"REMOVE ME!!!!!!!!!!!!!\")\n",
    "LIMIT = 4000 # remove this\n",
    "X = X[1:LIMIT, :] # remove this\n",
    "y = y[1:LIMIT] # remove this\n",
    "=#\n",
    "\n",
    "p = size(X)[2]\n",
    "\n",
    "# Splitting train and test sets\n",
    "train_prop = 0.8\n",
    "n = length(y)\n",
    "n_train = Int(round(train_prop*n))\n",
    "\n",
    "X_train = X[begin:n_train, :]\n",
    "X_test = X[n_train+1:end, :]\n",
    "y_train = y[begin:n_train]\n",
    "y_test = y[n_train+1:end]\n",
    "\n",
    "n_test = length(y_test);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Centering and rescaling features\n",
    "mean = Statistics.mean(X_train, dims=1)\n",
    "std = Statistics.std(X_train, dims=1)\n",
    "X = (X.-mean)./std\n",
    "X_train = (X_train.-mean)./std\n",
    "X_test = (X_test.-mean)./std;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing the covariance matrix\n",
    "c = (transpose(X_train)*X_train)/n\n",
    "\n",
    "# Computing eigenvalues and eigenvectors\n",
    "eig = eigen(c)\n",
    "eigenvectors = eig.vectors[:, end:-1:begin]\n",
    "eigenvalues = reverse(eig.values);\n",
    "\n",
    "# Plotting the eigenvalue spectrum\n",
    "plot(eigenvalues,\n",
    "    seriestype=:scatter,\n",
    "    label=\"Eigenvalues\",\n",
    "    xticks=1:length(eigenvalues),\n",
    "    #yscale=:log2,\n",
    "    #yticks=[10.0^i for i in ceil(log10(maximum(eigenvalues))):-1:floor(log10(minimum(eigenvalues)))],\n",
    "    title = \"Eigenvalue spectrum\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing projection on principal components\n",
    "pc_train = X_train*eigenvectors;\n",
    "\n",
    "# Plotting data projected on first two principal components\n",
    "function plot_PCs(pc, labels)\n",
    "    distinguishable_colors = (\"blue\", \"crimson\")\n",
    "\n",
    "    classes = sort(unique(labels))\n",
    "    color_dict = Dict(c => distinguishable_colors[i] for (i,c) in enumerate(classes))\n",
    "    colors = map(class -> color_dict[class], labels)\n",
    "\n",
    "    pc_plot = plot(\n",
    "        pc[:, 1],\n",
    "        pc[:, 2],\n",
    "        color = colors,\n",
    "        alpha=.75,\n",
    "        xlabel=\"PC1\",\n",
    "        ylabel=\"PC2\",\n",
    "        seriestype=:scatter,\n",
    "        title = \"Projection on first two PCs\",\n",
    "        label=nothing\n",
    "    )\n",
    "\n",
    "    # The following is a trick done in order to show the legend\n",
    "    for class in classes\n",
    "        indexes = (1:length(labels))[labels .== class]\n",
    "\n",
    "        plot!(\n",
    "            pc[indexes[1:2], 1],\n",
    "            pc[indexes[1:2], 2],\n",
    "            color=color_dict[class],\n",
    "            label = string(class),\n",
    "            seriestype=:scatter,\n",
    "            alpha=0.75,\n",
    "            legend = :outertopright)\n",
    "    end\n",
    "\n",
    "    return pc_plot\n",
    "\n",
    "end\n",
    "\n",
    "plot_PCs(pc_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic regression\n",
    "function I(a, b)\n",
    "    p = freqtable(DataFrame(a = a, b = b), :a, :b) / length(a)\n",
    "    n_classes_a = length(unique(a))\n",
    "    n_classes_b = length(unique(b))\n",
    "    sum([p[i,j]>0. ? p[i,j]*log(p[i,j]/(sum(p[i,:])*sum(p[:,j]))) : 0 for i=1:n_classes_a, j=1:n_classes_b])\n",
    "end\n",
    "\n",
    "function NMI(a, b)\n",
    "    p = freqtable(DataFrame(a = a, b = b), :a, :b) / length(a)\n",
    "    n_classes_a = length(unique(a))\n",
    "    n_classes_b = length(unique(b))\n",
    "    mi = sum([p[i,j]>0. ? p[i,j]*log(p[i,j]/(sum(p[i,:])*sum(p[:,j]))) : 0 for i=1:n_classes_a, j=1:n_classes_b])\n",
    "    \n",
    "    p_a = sum(p[1, :])\n",
    "    p_b = sum(p[:, 1])\n",
    "    h_a = p_a > 0 ? -p_a * log(p_a) : 0.\n",
    "    h_b = p_b > 0 ? -p_b * log(p_b) : 0.\n",
    "\n",
    "    return 2*mi/(p_a + p_b)\n",
    "end\n",
    "\n",
    "function accuracy(a, b)\n",
    "    p = freqtable(DataFrame(a = a, b = b), :a, :b) / length(a)\n",
    "    n_classes_a = length(unique(a))\n",
    "    n_classes_b = length(unique(b))\n",
    "    sum([p[i, i] for i=1:n_classes_b])\n",
    "end\n",
    "\n",
    "function logistic_regression_experiment(pc_train, pc_test, y_train, y_test, components_range)\n",
    "    mi = zeros(components_range)\n",
    "    #acc = zeros(pc_limit)\n",
    "\n",
    "    for i in 1:components_range\n",
    "        x_train = hcat(ones(size(pc_train)[1]), pc_train[:,begin:i]) # I have to add the bias manually...\n",
    "        x_test = hcat(ones(size(pc_test)[1]), pc_test[:,begin:i]) # I have to add the bias manually...\n",
    "        model = glm(x_train, y_train, Bernoulli())\n",
    "        y_predicted = predict(model, x_test).>0.5\n",
    "        mi[i] = NMI(y_test, y_predicted)\n",
    "        #acc[i] = accuracy(y_test, y_predicted)\n",
    "    end\n",
    "\n",
    "    plot(mi,\n",
    "        seriestype=:scatter,\n",
    "        label=nothing,\n",
    "        #xticks=1:length(mi),\n",
    "        ylims=(0.,1.),\n",
    "        xlabel = \"Number of PCs\",\n",
    "        ylabel = \"Mutual information\",\n",
    "        title = \"Prediction's mutual information\")\n",
    "end\n",
    "\n",
    "# Getting test set projection on PCs\n",
    "pc_test = X_test*eigenvectors;\n",
    "logistic_regression_experiment(pc_train, pc_test, y_train, y_test, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing the Kernel matrix\n",
    "\n",
    "# Computing l2 Kernel matrix\n",
    "l2_K(m) = (sum((reshape(m, (1, size(m)[1], size(m)[2])) .- reshape(m, (size(m)[1], 1, size(m)[2]))).^2f0, dims=3))[:,:,1]\n",
    "l2_K_matrix = l2_K(X);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing Gaussian Kernel matrix\n",
    "Gaussian_K(l2_K_matrix, width) = exp.(-0.5f0 .* l2_K_matrix ./ (width^2f0))\n",
    "\n",
    "width = 1.0f0\n",
    "K = Gaussian_K(l2_K_matrix, width);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear PCA Kernel\n",
    "K = X*transpose(X);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Gram matrix by double Centering\n",
    "function double_centering(M)\n",
    "    n = Float32(size(M)[1])\n",
    "    return -0.5f0*(M .-sum(M, dims=1)./n .-sum(M, dims=2)./n .+ sum(M)./(n^2f0))\n",
    "end\n",
    "\n",
    "# G = double_centering(K);\n",
    "println(\"CHANGE THIS!!!!!\")\n",
    "G = K; # change this!!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing eigenvalues and eigenvectors\n",
    "kernel_eig = eigen(Symmetric(G/size(G)[1]))\n",
    "kernel_eigenvectors = kernel_eig.vectors[:, end:-1:begin]\n",
    "kernel_eigenvalues = reverse(kernel_eig.values);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the eigenvalue spectrum\n",
    "plot(kernel_eigenvalues,\n",
    "    seriestype=:scatter,\n",
    "    label=\"Kernel Eigenvalues\",\n",
    "    #xticks=1:length(eigenvalues),\n",
    "    #yscale=:log10,\n",
    "    #yticks=[10.0^i for i in ceil(log10(maximum(eigenvalues))):-1:floor(log10(minimum(eigenvalues)))],\n",
    "    title = \"Eigenvalue spectrum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing projection on principal components\n",
    "# HOW ???\n",
    "#pc_train = G*eigenvectors;\n",
    "kernel_pc_train = kernel_eigenvectors[begin:n_train, :];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting train data projected on first two principal components\n",
    "plot_PCs(kernel_pc_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "\n",
    "# Computing PCs projection for test set\n",
    "kernel_pc_test = kernel_eigenvectors[n_train+1:end, :]; # select only test data\n",
    "\n",
    "# Plotting data projected on first two principal components\n",
    "plot_PCs(kernel_pc_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_regression_experiment(kernel_pc_train, kernel_pc_test, y_train, y_test, 20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.5",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
